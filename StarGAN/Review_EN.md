# StarGAN 논문 리뷰
[About Code](https://github.com/HanGyuTak/Thesis-Review/blob/main/StarGAN/Code.md)

## Abstract

* Recent approaches have shown effective performance on image transformation between two domains, but have limitations in handling three or more domains.
* To address this issue, StarGAN is proposed, which uses one model to perform image transformation for multiple domains.
* Multiple datasets (RaFD, CelebA) are trained on multiple domains within one network.
* This enables flexible transformation of input images to desired domains.

> ## 1. Introduction

- Perform training to convert from domain A to B using training data from two different domains.
- Extract multiple domains from images such as hair color, gender, and age.
  - CelebA: Extract attributes such as hair color, gender, and age.
  - RaFD: Extract emotion domains such as happiness, anger, and sadness.
- Train on RaFD to change the expression of CelebA images, by learning the two datasets together.

![IMG_E790BB5F1968-1](https://user-images.githubusercontent.com/50629765/219863839-0c3d675b-a9dd-43a1-ae0c-8789f3025ac9.jpeg)

* The existing model was inefficient and ineffective in transforming between multiple domains.
  * If there were k domains, k(k-1) generators had to be trained, making it inefficient.
  * Each generator could not utilize the entire dataset.
* StarGAN learns the mapping, or connection, between multiple domains using a single generator.
  * Given two images and domain information, it learns the transformation between corresponding domains.
  * Unnecessary labels are ignored, and only specific labels extracted from the dataset are learned.

> ## 2. Related Work

* GAN (Generative Adversarial Networks)
* Conditional GANs
* Img-to-Img Translation (CycleGAN)
  * Learning the relationship between two domains in a single model

> ## 3. Star Generative Adversarial Networks

## 3 - 1. Muti-Domain Image-to-Image Translation

* The goal is to train a single G(generator) to perform mapping between multiple domains.
* Given an input image x, an output image y, and a target domain label c: [G(x, c) -> y].
* Randomly generating c enables G to flexibly transform images.
* A single D(discriminator) is supported by an auxiliary classifier to control multiple domains.
* D: x → {Dsrc(x), Dcls(x)}. 
  
### Adversarial Loss 

![IMG_57FC6DAEAC7A-1](https://user-images.githubusercontent.com/50629765/219863865-89884597-4d1c-4371-988f-08c97afbf0e9.jpeg)


* G aims to minimize the loss, while D aims to maximize it.
  
### Domain Classification Loss

* To ensure that the image generated by G(x,c) is classified as c, a classifier is added on top of D. 
* The domain classification loss of the real images is used to optimize D.

![IMG_DD09A5BA8DC8-1](https://user-images.githubusercontent.com/50629765/219863884-b591a08f-989c-460a-b5c0-6a2e04da097b.jpeg)

* Optimizing D using domain classification loss of real images
* D cls(c′|x) is the probability distribution of domain labels calculated by D.
* D learns to classify (real Img x with label c') by minimizing this loss.

![IMG_F4ECA0F27C1F-1](https://user-images.githubusercontent.com/50629765/219863889-f1be11ce-18cd-4b66-a968-66573beb0be1.jpeg)

* G aims to minimize this loss to make the generated image classified as the target domain c.

### Reconstruction Loss

* Even if the above loss is minimized, there are difficulties in preserving the non-transformed domains in the input image.

![IMG_72F4C97DFD50-1](https://user-images.githubusercontent.com/50629765/219863895-5c768829-1131-4f4d-9a04-8c595500e1f9.jpeg)

* Apply cycle consistency loss
* Provide G with the transformed image G(x, c) and the original domain c' as inputs → Reconstruct original image
* Use G twice in total (original → transformed image → original reconstruction)

### Full Objective

* optimize Fuctions of G and D
  
![IMG_4F814845022B-1](https://user-images.githubusercontent.com/50629765/219863898-35a4e46b-de00-4192-b902-60bce54c374a.jpeg)

* λcls and λrec are hyperparameters that affect the classification and reconstruction loss.

## 3 - 2. Training with Multiple Datasets

* CelebA dataset has attributes such as hair color, while RaFD dataset has expression attributes.
* In such multi-dataset scenarios, there is a problem of requiring c' labels necessary for reconstruction in G(x, c).

### Mask Vector

* Introducing the "make vector" focuses on specific labels while ignoring unspecified labels.

![mask_vecotr](https://user-images.githubusercontent.com/50629765/219866136-663ceab6-d40f-4080-a6fe-cc7df648216e.jpeg)

* "c" represents the label vector of the i-th dataset.

### Training Strategy

* When training with multiple datasets, domain labels are defined as mask vectors and inputted into G.
* The structure of G is no different from training with a single dataset.
* To create probability distributions for all datasets, D's auxiliary classifier is expanded.
* D learns to minimize classification errors for specific labels through the process of minimizing the classification loss of labels that exist in CelebA images during training.
* Training alternates between RaFD and CelebA to learn all labels.

> ## 4. Implementation

### Imporve GAN Training

![image](https://user-images.githubusercontent.com/50629765/224294892-b999b941-b5e9-4999-9484-a957a639e620.png)

* The gradient penalty defined by the above equation is used in Wasserstein GAN.

### Network Acchitecture

* Generator network configuration of StarGAN adopted by CycleGAN
  * convolutional layer (stride size : 2) -> downsampling
  * Residual Block of 6
  * transposed convolutional layer (stride size : 2) -> upsampling
* Apply standardization to G only

> ## 5. Experiment

### 5.1. Baseline Models

* DIAT - image-to-image transform (2 domain)
  * CycleGAN learns the mapping between two domain images X and Y
  * It uses the normalization technique of |x - F(G(x))| to preserve the original characteristics of the input images.
* CycleGAN - image-to-image transform (2 domain)
  * Enter G(x) back into G to measure loss with the original image
* IcGAN - By cGAN
  * Mapping G : {z, c} → x -> Reverse Mapping Ez : x → z 및 Ec : x → c Train
  * Synthesize images by only changing condition vectors while preserving latent vectors

### 5.2. Datasets

* CelebA
  * 7 domains of celebrity face data
  * Hair color (black, blonde, brown), Gender, Age
* RaFD
  * 8 expression data from participants

### 5.3. Training

* Optimizer : Adam
* batch size : 16
* CelebA
  * learning rate : (epoch) (0 ~ 10) 0.0001, (11 ~ ) 0까지 점차 감소
* RaFD
  * learning rate : (epoch) (0 ~ 100) 0.0001, (11 ~ ) 0까지 점차 감소
* GPU : NVIDIA Tesla M40

### 5.4. Experimental Results on CelebA

* Learn all pairs of labels with cross-domain models such as DIAT and CycleGAN
* Evaluate
  * Instead of learning fixed transformations, StarGAN learns to be flexible based on labels for target domains
  * Better performance than other r-cross domains
  * Maintain facial identity over IcGAN
  * Map enabled in convolutional layer is saved as late presentation
  * late response : a numerical array representing the features found in the image

![IMG_78CC5B591D7C-1](https://user-images.githubusercontent.com/50629765/224294980-cb4ed0b4-e434-4439-a4da-daeacbf76501.jpeg)
![IMG_ADAA2C15EAE7-1](https://user-images.githubusercontent.com/50629765/224295011-bb1cfb6f-a118-4d86-8450-319729932c8e.jpeg)

### 5.5. Experimental Results on RaFD

* Learn RaFD's various facial expressions by inputting expressionless
* Approximately 500 pieces of data per domain
* Evaluation
  * StarGAN models create the most natural facial expressions
  * DIAT and CycleGAN maintain the identity of the input image but reduce clarity
  * IcGAN even fails to maintain its identity
  * Why?
    * The other model learns 1000 pieces of data when learning 2 domain
    * StarGAN uses approximately 4000 pieces of data from all domains

![IMG_828603DA420E-1](https://user-images.githubusercontent.com/50629765/224295058-2a43d1ef-60eb-4af9-a95a-e8f3dbadc859.jpeg)


* Image classification results generated after classifier learning with RaFD

![RaFD cls](https://user-images.githubusercontent.com/50629765/224295099-5e4b4a1d-7fa9-47b2-8089-9f2934bfc68f.jpeg)


* StarGAN has the smallest classification error
* Parameters required for learning are significantly smaller than other models
  * -> Because StarGAN uses only one pair (G, D)

### 5.6. Experimental Results on CelebA + RaFD

![IMG_0D6FA5D07ACF-1](https://user-images.githubusercontent.com/50629765/224295184-4d2ad2c0-bd7f-4449-8965-c2edcc81b9f4.jpeg)

* Using both CelebA and RaFD datasets (with mask vector)
* JNT(jointly train), SNG(single train)
* Effect of joint training
  * JNT creates a clearer quality expression
  * SNG learns image translation of CelebA as X
